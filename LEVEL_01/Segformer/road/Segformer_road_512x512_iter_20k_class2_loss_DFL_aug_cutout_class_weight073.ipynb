{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30412,"status":"ok","timestamp":1653445048029,"user":{"displayName":"양우민","userId":"09434565168064209811"},"user_tz":-540},"id":"nllMQ6RCsenv","outputId":"c7f95561-ba20-4655-8138-3ecf5c9ff185"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13074,"status":"ok","timestamp":1653445061098,"user":{"displayName":"양우민","userId":"09434565168064209811"},"user_tz":-540},"id":"PPb5e_glt9i6","outputId":"8205f209-645a-44c3-8c4c-d69243c6e69a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n","Collecting mmcv-full\n","  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/mmcv_full-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (46.6 MB)\n","\u001b[K     |████████████████████████████████| 46.6 MB 123 kB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (21.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (3.13)\n","Collecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (1.21.6)\n","Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (4.1.2.30)\n","Collecting yapf\n","  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n","\u001b[K     |████████████████████████████████| 190 kB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full) (3.0.9)\n","Installing collected packages: yapf, addict, mmcv-full\n","Successfully installed addict-2.4.0 mmcv-full-1.5.1 yapf-0.32.0\n","Cloning into 'mmsegmentation'...\n","remote: Enumerating objects: 7170, done.\u001b[K\n","remote: Counting objects: 100% (182/182), done.\u001b[K\n","remote: Compressing objects: 100% (142/142), done.\u001b[K\n","remote: Total 7170 (delta 49), reused 159 (delta 40), pack-reused 6988\u001b[K\n","Receiving objects: 100% (7170/7170), 13.42 MiB | 31.60 MiB/s, done.\n","Resolving deltas: 100% (5231/5231), done.\n"]}],"source":["!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n","!git clone https://github.com/open-mmlab/mmsegmentation.git"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3067,"status":"ok","timestamp":1653445064157,"user":{"displayName":"양우민","userId":"09434565168064209811"},"user_tz":-540},"id":"zU2hdo-iuDM_","outputId":"94781d88-e4d9-4feb-d90e-f0dbfb059a42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setup complete. Using torch 1.11.0+cu113 (Tesla P100-PCIE-16GB)\n"]}],"source":["import torch\n","\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1653445064157,"user":{"displayName":"양우민","userId":"09434565168064209811"},"user_tz":-540},"id":"5Y38Lq6KuG75","outputId":"a7afe06a-81b5-4bf0-e534-97c1529c9949"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/mmsegmentation\n"]}],"source":["%cd mmsegmentation"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"scnWNR9VuKnB","executionInfo":{"status":"ok","timestamp":1653445066193,"user_tz":-540,"elapsed":2038,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n","import mmcv"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nZ7kLaEfuMhh","executionInfo":{"status":"ok","timestamp":1653445066194,"user_tz":-540,"elapsed":4,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["import os.path as osp\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23386,"status":"ok","timestamp":1653445089577,"user":{"displayName":"양우민","userId":"09434565168064209811"},"user_tz":-540},"id":"O5uVMzGxuRlJ","outputId":"cc1f682d-3cd0-47f9-a311-66d62a22f2e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-25 02:17:49--  https://download.openmmlab.com/mmsegmentation/v0.5/segformer/segformer_mit-b5_8x1_1024x1024_160k_cityscapes/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth\n","Resolving download.openmmlab.com (download.openmmlab.com)... 47.88.36.72\n","Connecting to download.openmmlab.com (download.openmmlab.com)|47.88.36.72|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 328290585 (313M) [application/octet-stream]\n","Saving to: ‘//content/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth’\n","\n","//content/mmsegment 100%[===================>] 313.08M  13.1MB/s    in 22s     \n","\n","2022-05-25 02:18:12 (14.1 MB/s) - ‘//content/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth’ saved [328290585/328290585]\n","\n"]}],"source":["!mkdir checkpoints\n","!wget -O //content/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth https://download.openmmlab.com/mmsegmentation/v0.5/segformer/segformer_mit-b5_8x1_1024x1024_160k_cityscapes/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"km0JLIuRumCX","executionInfo":{"status":"ok","timestamp":1653445089578,"user_tz":-540,"elapsed":9,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["from mmseg.datasets.builder import DATASETS\n","from mmseg.datasets.custom import CustomDataset\n","\n","classes = ('background', 'road')\n","palette = [[0, 0, 0], [255, 255, 0]]\n","\n","@DATASETS.register_module()\n","class SIADataset(CustomDataset):\n","  CLASSES = classes\n","  PALETTE = palette\n","  def __init__(self, split, **kwargs):\n","    super().__init__(img_suffix='.png', seg_map_suffix='.png', \n","                     split=split, **kwargs)\n","    assert osp.exists(self.img_dir) and self.split is not None"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1653445090061,"user":{"displayName":"양우민","userId":"09434565168064209811"},"user_tz":-540},"id":"Qx5ugOgwuxGW","outputId":"1ab596c9-3bca-405e-a781-5e043efe8f55"},"outputs":[{"output_type":"stream","name":"stdout","text":["norm_cfg = dict(type='SyncBN', requires_grad=True)\n","model = dict(\n","    type='EncoderDecoder',\n","    pretrained=None,\n","    backbone=dict(\n","        type='MixVisionTransformer',\n","        in_channels=3,\n","        embed_dims=64,\n","        num_stages=4,\n","        num_layers=[3, 6, 40, 3],\n","        num_heads=[1, 2, 5, 8],\n","        patch_sizes=[7, 3, 3, 3],\n","        sr_ratios=[8, 4, 2, 1],\n","        out_indices=(0, 1, 2, 3),\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        drop_rate=0.0,\n","        attn_drop_rate=0.0,\n","        drop_path_rate=0.1,\n","        init_cfg=dict(type='Pretrained', checkpoint='pretrain/mit_b5.pth')),\n","    decode_head=dict(\n","        type='SegformerHead',\n","        in_channels=[64, 128, 320, 512],\n","        in_index=[0, 1, 2, 3],\n","        channels=256,\n","        dropout_ratio=0.1,\n","        num_classes=19,\n","        norm_cfg=dict(type='SyncBN', requires_grad=True),\n","        align_corners=False,\n","        loss_decode=dict(\n","            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n","    train_cfg=dict(),\n","    test_cfg=dict(mode='slide', crop_size=(1024, 1024), stride=(768, 768)))\n","dataset_type = 'CityscapesDataset'\n","data_root = 'data/cityscapes/'\n","img_norm_cfg = dict(\n","    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n","crop_size = (1024, 1024)\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations'),\n","    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n","    dict(type='RandomCrop', crop_size=(1024, 1024), cat_max_ratio=0.75),\n","    dict(type='RandomFlip', prob=0.5),\n","    dict(type='PhotoMetricDistortion'),\n","    dict(\n","        type='Normalize',\n","        mean=[123.675, 116.28, 103.53],\n","        std=[58.395, 57.12, 57.375],\n","        to_rgb=True),\n","    dict(type='Pad', size=(1024, 1024), pad_val=0, seg_pad_val=255),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(2048, 1024),\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img'])\n","        ])\n","]\n","data = dict(\n","    samples_per_gpu=1,\n","    workers_per_gpu=1,\n","    train=dict(\n","        type='CityscapesDataset',\n","        data_root='data/cityscapes/',\n","        img_dir='leftImg8bit/train',\n","        ann_dir='gtFine/train',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(type='LoadAnnotations'),\n","            dict(\n","                type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),\n","            dict(\n","                type='RandomCrop', crop_size=(1024, 1024), cat_max_ratio=0.75),\n","            dict(type='RandomFlip', prob=0.5),\n","            dict(type='PhotoMetricDistortion'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='Pad', size=(1024, 1024), pad_val=0, seg_pad_val=255),\n","            dict(type='DefaultFormatBundle'),\n","            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n","        ]),\n","    val=dict(\n","        type='CityscapesDataset',\n","        data_root='data/cityscapes/',\n","        img_dir='leftImg8bit/val',\n","        ann_dir='gtFine/val',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(2048, 1024),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ]),\n","    test=dict(\n","        type='CityscapesDataset',\n","        data_root='data/cityscapes/',\n","        img_dir='leftImg8bit/val',\n","        ann_dir='gtFine/val',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(2048, 1024),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ]))\n","log_config = dict(\n","    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n","dist_params = dict(backend='nccl')\n","log_level = 'INFO'\n","load_from = None\n","resume_from = None\n","workflow = [('train', 1)]\n","cudnn_benchmark = True\n","optimizer = dict(\n","    type='AdamW',\n","    lr=6e-05,\n","    betas=(0.9, 0.999),\n","    weight_decay=0.01,\n","    paramwise_cfg=dict(\n","        custom_keys=dict(\n","            pos_block=dict(decay_mult=0.0),\n","            norm=dict(decay_mult=0.0),\n","            head=dict(lr_mult=10.0))))\n","optimizer_config = dict()\n","lr_config = dict(\n","    policy='poly',\n","    warmup='linear',\n","    warmup_iters=1500,\n","    warmup_ratio=1e-06,\n","    power=1.0,\n","    min_lr=0.0,\n","    by_epoch=False)\n","runner = dict(type='IterBasedRunner', max_iters=160000)\n","checkpoint_config = dict(by_epoch=False, interval=16000)\n","evaluation = dict(interval=16000, metric='mIoU', pre_eval=True)\n","\n"]}],"source":["# config 파일을 설정하고, 다운로드 받은 pretrained 모델을 checkpoint로 설정. \n","config_file = '/content/mmsegmentation/configs/segformer/segformer_mit-b5_8x1_1024x1024_160k_cityscapes.py'\n","checkpoint_file = '/content/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth'\n","\n","from mmcv import Config\n","\n","cfg = Config.fromfile(config_file)\n","print(cfg.pretty_text)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"rWBsQMhXvA4T","executionInfo":{"status":"ok","timestamp":1653445090061,"user_tz":-540,"elapsed":7,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["model = dict(\n","    type='EncoderDecoder',\n","    pretrained='open-mmlab://resnet101_v1c',\n","    backbone=dict(\n","        type='ResNetV1c',\n","        depth=101,\n","        num_stages=4,\n","        out_indices=(0, 1, 2, 3),\n","        dilations=(1, 1, 1, 1),\n","        strides=(1, 2, 2, 2),\n","        norm_cfg=dict(type='BN', requires_grad=True),\n","        norm_eval=False,\n","        style='pytorch',\n","        contract_dilation=True),\n","    neck=dict(\n","        type='FPN',\n","        in_channels=[256, 512, 1024, 2048],\n","        out_channels=256,\n","        num_outs=4),\n","    decode_head=dict(\n","        type='FPNHead',\n","        in_channels=[256, 256, 256, 256],\n","        in_index=[0, 1, 2, 3],\n","        feature_strides=[4, 8, 16, 32],\n","        channels=128,\n","        dropout_ratio=0.1,\n","        num_classes=19,\n","        norm_cfg=dict(type='BN', requires_grad=True),\n","        align_corners=False,\n","        loss_decode=dict(\n","            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n","    train_cfg=dict(),\n","    test_cfg=dict(mode='whole'))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"JVKVeenRvLWC","executionInfo":{"status":"ok","timestamp":1653445090062,"user_tz":-540,"elapsed":7,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["img_norm_cfg = dict(\n","    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n","crop_size = (512, 512)\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations'),\n","    dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 1.5)),\n","    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n","    dict(type='RandomFlip', prob=0.5),\n","    dict(type='PhotoMetricDistortion'),\n","    dict(\n","        type='Normalize',\n","        mean=[123.675, 116.28, 103.53],\n","        std=[58.395, 57.12, 57.375],\n","        to_rgb=True),\n","    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(1024, 1024),\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img'])\n","        ])\n","]"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"RR2MIwc7vP7T","executionInfo":{"status":"ok","timestamp":1653445090062,"user_tz":-540,"elapsed":7,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["dataset_type = 'CityscapesDataset'\n","data_root = 'data/cityscapes/'\n","\n","data = dict(\n","    samples_per_gpu=2,  #batch size\n","    workers_per_gpu=2,  \n","    train=dict(\n","        type='CityscapesDataset',\n","        data_root='data/cityscapes/',\n","        img_dir='leftImg8bit/train',\n","        ann_dir='gtFine/train',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(type='LoadAnnotations'),\n","            dict(\n","                type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 1.5)),\n","            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n","            dict(type='RandomFlip', prob=0.5),\n","            dict(type='PhotoMetricDistortion'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n","            dict(type='DefaultFormatBundle'),\n","            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n","        ]),\n","    val=dict(\n","        type='CityscapesDataset',\n","        data_root='data/cityscapes/',\n","        img_dir='leftImg8bit/val',\n","        ann_dir='gtFine/val',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1024, 1024),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ]),\n","    test=dict(\n","        type='CityscapesDataset',\n","        data_root='data/cityscapes/',\n","        img_dir='leftImg8bit/val',\n","        ann_dir='gtFine/val',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1024, 1024),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ]))"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"caVFRT7QvWE8","executionInfo":{"status":"ok","timestamp":1653445090062,"user_tz":-540,"elapsed":7,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["cfg.norm_cfg = dict(type='BN', requires_grad=True)\n","#cfg.model.backbone.norm_cfg = cfg.norm_cfg\n","cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n","\n","cfg.model.decode_head.num_classes = 2\n","\n","cfg.model.decode_head.loss_decode = [dict(type='DiceLoss', loss_weight = 1.0, class_weight=[0.001, 0.7, 0.299]),\n","                                     dict(type='FocalLoss', loss_weight = 1.0, class_weight=[0.001, 0.7, 0.299]),\n","                                     dict(type='LovaszLoss', loss_weight = 1.0, reduction='none',class_weight=[0.001, 0.7, 0.299])]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"X9IpatmxvYMt","executionInfo":{"status":"ok","timestamp":1653445090062,"user_tz":-540,"elapsed":7,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["cfg.img_norm_cfg = dict(\n","    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n","cfg.crop_size = (512, 512)\n","cfg.train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations'),\n","    dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),\n","    dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n","    dict(type='RandomFlip', flip_ratio=0.5),\n","    dict(type='RandomCutOut',prob=0.1,n_holes=(2,4),cutout_ratio=(0.1,0.3)),\n","    dict(type='PhotoMetricDistortion'),\n","    dict(type='Normalize', **cfg.img_norm_cfg),\n","    dict(type='Pad', size=cfg.crop_size, pad_val=0, seg_pad_val=255),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_semantic_seg']),\n","]\n","\n","cfg.val_pipeline = [\n","                    dict(type='LoadImageFromFile'),\n","                    dict(\n","                        type='MultiScaleFlipAug',\n","                        img_scale=(1024, 1024),\n","                        flip=False,\n","                        transforms=[\n","                                    dict(type='Resize', keep_ratio=True),\n","                                    dict(type='RandomFlip'),\n","                                    dict(\n","                                        type='Normalize',\n","                                        mean=[123.675, 116.28, 103.53],\n","                                        std=[58.395, 57.12, 57.375],\n","                                        to_rgb=True),\n","                                    dict(type='ImageToTensor', keys=['img']),\n","                                    dict(type='Collect', keys=['img'])\n","                                    ]),\n","                    \n","]\n","\n","cfg.test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(1024, 1024),\n","        img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            #dict(type='ResizeToMultiple', size_divisor=32),\n","            dict(type='RandomFlip'),\n","            dict(type='Normalize', **cfg.img_norm_cfg),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img']),\n","        ])\n","]"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"_4PNB6hGvds4","executionInfo":{"status":"ok","timestamp":1653445090063,"user_tz":-540,"elapsed":7,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["cfg.dataset_type = 'SIADataset'\n","cfg.data_root = '/content/drive/MyDrive/SIA'\n","\n","cfg.data.train.type = 'SIADataset'\n","cfg.data.train.data_root = '/content/drive/MyDrive/SIA'\n","cfg.data.train.img_dir = 'Data_set/road_dataset/img_dir/train'\n","cfg.data.train.ann_dir = 'Data_set/road_dataset/contour_ann_dir/train'\n","cfg.data.train.pipeline = cfg.train_pipeline\n","cfg.data.train.split = '/content/drive/MyDrive/SIA/Data_set/road_dataset/mask_dir/train/up_train_road.txt'\n","\n","cfg.data.val.type = 'SIADataset'\n","cfg.data.val.data_root = '/content/drive/MyDrive/SIA'\n","cfg.data.val.img_dir = 'Data_set/road_dataset/img_dir/train'\n","cfg.data.val.ann_dir = 'Data_set/road_dataset/contour_ann_dir/train'\n","cfg.data.val.pipeline = cfg.test_pipeline\n","cfg.data.val.split = '/content/drive/MyDrive/SIA/Data_set/road_dataset/mask_dir/train/up_val_road.txt'\n","\n","cfg.data.test.type = 'SIADataset'\n","cfg.data.test.data_root = '/content/drive/MyDrive/SIA'\n","cfg.data.test.img_dir = 'Data_set/road_dataset/img_dir/val'\n","cfg.data.test.ann_dir = 'Data_set/road_dataset/contour_ann_dir/val'\n","cfg.data.test.pipeline = cfg.test_pipeline\n","cfg.data.test.split = '/content/drive/MyDrive/SIA/Data_set/road_dataset/mask_dir/val/up.txt'"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"jXNN3BblvrfK","executionInfo":{"status":"ok","timestamp":1653445090063,"user_tz":-540,"elapsed":7,"user":{"displayName":"양우민","userId":"09434565168064209811"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"36e67bfa-0dfc-4ab4-fdf3-47fef08e14c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ncfg.data.train.type = 'SIADataset'\\ncfg.data.train.data_root = '/content/drive/MyDrive/SIA'\\ncfg.data.train.img_dir = 'images/up025'\\ncfg.data.train.ann_dir = 'labels'\\ncfg.data.train.pipeline = cfg.train_pipeline\\ncfg.data.train.split = '/content/drive/MyDrive/SIA/splits/road/train_road.txt'\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["'''\n","cfg.data.train.type = 'SIADataset'\n","cfg.data.train.data_root = '/content/drive/MyDrive/SIA'\n","cfg.data.train.img_dir = 'images/up025'\n","cfg.data.train.ann_dir = 'labels'\n","cfg.data.train.pipeline = cfg.train_pipeline\n","cfg.data.train.split = '/content/drive/MyDrive/SIA/splits/road/train_road.txt'\n","'''"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"l1Gc8ttSvw8e","executionInfo":{"status":"ok","timestamp":1653445090063,"user_tz":-540,"elapsed":6,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[],"source":["cfg.load_from = '/content/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth'\n","\n","# Set up working dir to save files and logs.\n","cfg.work_dir = '/content/drive/MyDrive/SIA/semantic_checkpoint/Segformer/road/DFL_class_weight073_cutout_class2'"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1046,"status":"ok","timestamp":1653445091103,"user":{"displayName":"양우민","userId":"09434565168064209811"},"user_tz":-540},"id":"vvSIfRurv0sm","outputId":"3cda1860-013d-4114-cc85-be078fed94ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Config:\n","norm_cfg = dict(type='BN', requires_grad=True)\n","model = dict(\n","    type='EncoderDecoder',\n","    pretrained=None,\n","    backbone=dict(\n","        type='MixVisionTransformer',\n","        in_channels=3,\n","        embed_dims=64,\n","        num_stages=4,\n","        num_layers=[3, 6, 40, 3],\n","        num_heads=[1, 2, 5, 8],\n","        patch_sizes=[7, 3, 3, 3],\n","        sr_ratios=[8, 4, 2, 1],\n","        out_indices=(0, 1, 2, 3),\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        drop_rate=0.0,\n","        attn_drop_rate=0.0,\n","        drop_path_rate=0.1,\n","        init_cfg=dict(type='Pretrained', checkpoint='pretrain/mit_b5.pth')),\n","    decode_head=dict(\n","        type='SegformerHead',\n","        in_channels=[64, 128, 320, 512],\n","        in_index=[0, 1, 2, 3],\n","        channels=256,\n","        dropout_ratio=0.1,\n","        num_classes=2,\n","        norm_cfg=dict(type='BN', requires_grad=True),\n","        align_corners=False,\n","        loss_decode=[\n","            dict(\n","                type='DiceLoss',\n","                loss_weight=1.0,\n","                class_weight=[0.001, 0.7, 0.299]),\n","            dict(\n","                type='FocalLoss',\n","                loss_weight=1.0,\n","                class_weight=[0.001, 0.7, 0.299]),\n","            dict(\n","                type='LovaszLoss',\n","                loss_weight=1.0,\n","                reduction='none',\n","                class_weight=[0.001, 0.7, 0.299])\n","        ]),\n","    train_cfg=dict(),\n","    test_cfg=dict(mode='slide', crop_size=(1024, 1024), stride=(768, 768)))\n","dataset_type = 'SIADataset'\n","data_root = '/content/drive/MyDrive/SIA'\n","img_norm_cfg = dict(\n","    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n","crop_size = (512, 512)\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations'),\n","    dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),\n","    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n","    dict(type='RandomFlip', flip_ratio=0.5),\n","    dict(\n","        type='RandomCutOut', prob=0.1, n_holes=(2, 4),\n","        cutout_ratio=(0.1, 0.3)),\n","    dict(type='PhotoMetricDistortion'),\n","    dict(\n","        type='Normalize',\n","        mean=[123.675, 116.28, 103.53],\n","        std=[58.395, 57.12, 57.375],\n","        to_rgb=True),\n","    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(1024, 1024),\n","        img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img'])\n","        ])\n","]\n","data = dict(\n","    samples_per_gpu=1,\n","    workers_per_gpu=1,\n","    train=dict(\n","        type='SIADataset',\n","        data_root='/content/drive/MyDrive/SIA',\n","        img_dir='Data_set/road_dataset/img_dir/train',\n","        ann_dir='Data_set/road_dataset/contour_ann_dir/train',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(type='LoadAnnotations'),\n","            dict(\n","                type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),\n","            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n","            dict(type='RandomFlip', flip_ratio=0.5),\n","            dict(\n","                type='RandomCutOut',\n","                prob=0.1,\n","                n_holes=(2, 4),\n","                cutout_ratio=(0.1, 0.3)),\n","            dict(type='PhotoMetricDistortion'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n","            dict(type='DefaultFormatBundle'),\n","            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n","        ],\n","        split=\n","        '/content/drive/MyDrive/SIA/Data_set/road_dataset/mask_dir/train/up_train_road.txt'\n","    ),\n","    val=dict(\n","        type='SIADataset',\n","        data_root='/content/drive/MyDrive/SIA',\n","        img_dir='Data_set/road_dataset/img_dir/train',\n","        ann_dir='Data_set/road_dataset/contour_ann_dir/train',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1024, 1024),\n","                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ],\n","        split=\n","        '/content/drive/MyDrive/SIA/Data_set/road_dataset/mask_dir/train/up_val_road.txt'\n","    ),\n","    test=dict(\n","        type='SIADataset',\n","        data_root='/content/drive/MyDrive/SIA',\n","        img_dir='Data_set/road_dataset/img_dir/val',\n","        ann_dir='Data_set/road_dataset/contour_ann_dir/val',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1024, 1024),\n","                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ],\n","        split=\n","        '/content/drive/MyDrive/SIA/Data_set/road_dataset/mask_dir/val/up.txt')\n",")\n","log_config = dict(\n","    interval=100, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n","dist_params = dict(backend='nccl')\n","log_level = 'INFO'\n","load_from = '/content/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth'\n","resume_from = None\n","workflow = [('train', 1)]\n","cudnn_benchmark = True\n","optimizer = dict(\n","    type='AdamW',\n","    lr=6e-05,\n","    betas=(0.9, 0.999),\n","    weight_decay=0.01,\n","    paramwise_cfg=dict(\n","        custom_keys=dict(\n","            pos_block=dict(decay_mult=0.0),\n","            norm=dict(decay_mult=0.0),\n","            head=dict(lr_mult=10.0))))\n","optimizer_config = dict()\n","lr_config = dict(\n","    policy='poly',\n","    warmup='linear',\n","    warmup_iters=1500,\n","    warmup_ratio=1e-06,\n","    power=1.0,\n","    min_lr=0.0,\n","    by_epoch=False)\n","runner = dict(type='IterBasedRunner', max_iters=20000)\n","checkpoint_config = dict(by_epoch=False, interval=1000)\n","evaluation = dict(interval=1000, metric='mIoU', pre_eval=True)\n","val_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(1024, 1024),\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img'])\n","        ])\n","]\n","work_dir = '/content/drive/MyDrive/SIA/semantic_checkpoint/Segformer/road/DFL_class_weight073_cutout_class2'\n","seed = 0\n","gpu_ids = range(0, 1)\n","\n"]}],"source":["cfg.runner.max_iteTrs = 200\n","cfg.log_config.interval = 100\n","cfg.evaluation.interval = 1000  # 모델 학습시 평가를 몇 번째 iteration마다 할 것인지 지정\n","cfg.checkpoint_config.interval = 1000  # 모델 학습시 학습한 모델을 몇 번째 iteration마다 저장할 것인지 지정\n","\n","cfg.runner = dict(type='IterBasedRunner', max_iters=20000)  # Iteration으로 동작, Epoch로 동작하게 변경할 수도 있음\n","# cfg.runner = dict(type='EpochBasedRunner', max_epochs=4000)  # Epoch로 변경\n","cfg.workflow = [('train', 1)]\n","\n","# Set seed to facitate reproducing the result\n","cfg.seed = 0\n","#set_random_seed(0, deterministic=False)\n","cfg.gpu_ids = range(1)\n","\n","# Let's have a look at the final config used for training\n","print(f'Config:\\n{cfg.pretty_text}')"]},{"cell_type":"markdown","source":["### 모델 학습"],"metadata":{"id":"xP_db_FP_lnS"}},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8IlH2WxNwFFW","outputId":"7cd3484d-1b2f-48d7-f546-a3eff29f1c3f","executionInfo":{"status":"error","timestamp":1653445112930,"user_tz":-540,"elapsed":21829,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-25 02:18:16,153 - mmseg - INFO - Loaded 542 images\n","2022-05-25 02:18:29,141 - mmseg - INFO - Loaded 66 images\n","2022-05-25 02:18:29,146 - mmseg - INFO - load checkpoint from local path: /content/mmsegmentation/checkpoints/segformer_mit-b5_8x1_1024x1024_160k_cityscapes_20211206_072934-87a052ec.pth\n","2022-05-25 02:18:29,679 - mmseg - WARNING - The model and loaded state dict do not match exactly\n","\n","size mismatch for decode_head.conv_seg.weight: copying a param with shape torch.Size([19, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 256, 1, 1]).\n","size mismatch for decode_head.conv_seg.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([2]).\n","2022-05-25 02:18:29,687 - mmseg - INFO - Start running, host: root@4a50245f511e, work_dir: /content/drive/MyDrive/SIA/semantic_checkpoint/Segformer/road/DFL_class_weight073_cutout_class2\n","2022-05-25 02:18:29,689 - mmseg - INFO - Hooks will be executed in the following order:\n","before_run:\n","(VERY_HIGH   ) PolyLrUpdaterHook                  \n","(NORMAL      ) CheckpointHook                     \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_train_epoch:\n","(VERY_HIGH   ) PolyLrUpdaterHook                  \n","(LOW         ) IterTimerHook                      \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_train_iter:\n","(VERY_HIGH   ) PolyLrUpdaterHook                  \n","(LOW         ) IterTimerHook                      \n","(LOW         ) EvalHook                           \n"," -------------------- \n","after_train_iter:\n","(ABOVE_NORMAL) OptimizerHook                      \n","(NORMAL      ) CheckpointHook                     \n","(LOW         ) IterTimerHook                      \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","after_train_epoch:\n","(NORMAL      ) CheckpointHook                     \n","(LOW         ) EvalHook                           \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_val_epoch:\n","(LOW         ) IterTimerHook                      \n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","before_val_iter:\n","(LOW         ) IterTimerHook                      \n"," -------------------- \n","after_val_iter:\n","(LOW         ) IterTimerHook                      \n"," -------------------- \n","after_val_epoch:\n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","after_run:\n","(VERY_LOW    ) TextLoggerHook                     \n"," -------------------- \n","2022-05-25 02:18:29,691 - mmseg - INFO - workflow: [('train', 1)], max: 20000 iters\n","2022-05-25 02:18:29,694 - mmseg - INFO - Checkpoints will be saved to /content/drive/MyDrive/SIA/semantic_checkpoint/Segformer/road/DFL_class_weight073_cutout_class2 by HardDiskBackend.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-c07218ae94fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir_or_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m train_segmentor(model, datasets, cfg, distributed=False, validate=True,\n\u001b[0;32m---> 18\u001b[0;31m                 meta=dict(CLASSES=classes, PALETTE=palette))\n\u001b[0m","\u001b[0;32m/content/mmsegmentation/mmseg/apis/train.py\u001b[0m in \u001b[0;36mtrain_segmentor\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmcv/runner/iter_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loaders, workflow, max_iters, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_iters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0miter_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmcv/runner/iter_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_train_iter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.train_step() must return a dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmcv/parallel/data_parallel.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/segmentors/base.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data_batch, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0maveraging\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmcv/runner/fp16_utils.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m                                 f'method of those classes {supported_types}')\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp16_enabled'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mold_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# get the arg spec of the decorated method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/segmentors/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, img, img_metas, gt_semantic_seg)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         loss_decode = self._decode_head_forward_train(x, img_metas,\n\u001b[0;32m--> 144\u001b[0;31m                                                       gt_semantic_seg)\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_decode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py\u001b[0m in \u001b[0;36m_decode_head_forward_train\u001b[0;34m(self, x, img_metas, gt_semantic_seg)\u001b[0m\n\u001b[1;32m     86\u001b[0m         loss_decode = self.decode_head.forward_train(x, img_metas,\n\u001b[1;32m     87\u001b[0m                                                      \u001b[0mgt_semantic_seg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                                                      self.train_cfg)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_decode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'decode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/decode_heads/decode_head.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, inputs, img_metas, gt_semantic_seg, train_cfg)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \"\"\"\n\u001b[1;32m    203\u001b[0m         \u001b[0mseg_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_semantic_seg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mmcv/runner/fp16_utils.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                                 'method of nn.Module')\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fp16_enabled'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mold_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;31m# get the arg spec of the decorated method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0margs_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/decode_heads/decode_head.py\u001b[0m in \u001b[0;36mlosses\u001b[0;34m(self, seg_logit, seg_label)\u001b[0m\n\u001b[1;32m    254\u001b[0m                     \u001b[0mseg_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseg_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                     ignore_index=self.ignore_index)\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 loss[loss_decode.loss_name] += loss_decode(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/losses/focal_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target, weight, avg_factor, reduction_override, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mvalid_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 avg_factor=avg_factor)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/mmsegmentation/mmseg/models/losses/focal_loss.py\u001b[0m in \u001b[0;36msigmoid_focal_loss\u001b[0;34m(pred, target, one_hot_target, weight, gamma, alpha, class_weight, valid_mask, reduction, avg_factor)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Function.apply does not accept keyword arguments, so the decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# \"weighted_loss\" is not applicable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mfinal_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# _sigmoid_focal_loss doesn't accept alpha of list type. Therefore, if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."]}],"source":["from mmseg.datasets import build_dataset\n","from mmseg.models import build_segmentor\n","from mmseg.apis import train_segmentor\n","\n","# Build the dataset\n","datasets = [build_dataset(cfg.data.train)]\n","\n","# Build the detector\n","model = build_segmentor(\n","    cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n","\n","# Add an attribute for visualization convenience\n","model.CLASSES = datasets[0].CLASSES\n","\n","# Create work_dir\n","mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n","train_segmentor(model, datasets, cfg, distributed=False, validate=True,\n","                meta=dict(CLASSES=classes, PALETTE=palette))"]},{"cell_type":"markdown","source":["### 모델 결과 시각화"],"metadata":{"id":"Pre69QNK_dwU"}},{"cell_type":"code","source":["img_result = ['BLD00002_PS3_K3A_NIA0276.png',\n","              'BLD00010_PS3_K3A_NIA0276.png',\n","              'BLD00047_PS3_K3A_NIA0276.png',\n","              'BLD12071_PS3_K3A_NIA0391.png',\n","              'BLD11907_PS3_K3A_NIA0391.png',\n","              'BLD11611_PS3_K3A_NIA0390.png',\n","              'BLD11474_PS3_K3A_NIA0390.png',\n","              'BLD10413_PS3_K3A_NIA0388.png',\n","              'BLD10150_PS3_K3A_NIA0388.png',\n","              'BLD00836_PS3_K3A_NIA0277.png']\n","\n","checkpoint_file = '/content/drive/MyDrive/SIA/semantic_checkpoint/Segformer/road/DFL_class_weight073_cutout_class2/iter_20000.pth'  #학습된 모델\n","\n","\n","# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용. \n","model_ckpt = init_segmentor(cfg, checkpoint_file, device='cuda:0')  #cuda gpu 사용\n","\n","for ir in range(len(img_result)):\n","  img = mmcv.imread('/content/drive/MyDrive/SIA/Data_set/road_dataset/img_dir/val/'+img_result[ir])\n","  result = inference_segmentor(model_ckpt, img)\n","  show_result_pyplot(model_ckpt, img, result, palette)\n"],"metadata":{"id":"v2VyaZksCGEP","executionInfo":{"status":"aborted","timestamp":1653445112929,"user_tz":-540,"elapsed":5,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","file_list = os.listdir('/content/drive/MyDrive/SIA/Data_set/road_dataset/img_dir/val')\n","test_png = []\n","for i in file_list:\n","  if '.png' in i:\n","    test_png.append(i)\n","\n","for j in test_png:\n","  img = mmcv.imread('/content/drive/MyDrive/SIA/Data_set/road_dataset/img_dir/val/'+j)\n","  result = inference_segmentor(model_ckpt, img)\n","  show_result_pyplot(model_ckpt, img, result, palette)"],"metadata":{"id":"2Kh7-rsWAAi0","executionInfo":{"status":"aborted","timestamp":1653445112929,"user_tz":-540,"elapsed":4,"user":{"displayName":"양우민","userId":"09434565168064209811"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Segformer_road_512x512_iter_20k_class2_loss_DFL_aug_cutout_class_weight073.ipynb","provenance":[{"file_id":"1-tUNSqxwGyamS6a3CkRHoXF5GMszfber","timestamp":1652320453219}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}